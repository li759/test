[[network]]

温度参数 α 的损失的作用是**自动调整熵正则化项的权重**，以确保策略的熵达到目标值。
具体作用：
- **自动调整熵**：通过优化 α，算法能够自动调整熵正则化项权重，使策略熵接近目标值。
- **动态平衡探索与利用**：根据环境的复杂性和当前策略性能，动态调整探索与利用平衡。
温度参数 α 的损失通常定义为： $L_α​=−E_{s∼D}​[α(logπ(a∣s)+target\_entropy)]$
其中：
- α 是温度参数。α 的值会根据策略的熵动态调整。如果策略的熵过高（即策略过于随机），α 会减小，减少熵正则化项的权重，使策略更倾向于利用已知的最优动作。反之，如果策略的熵过低（即策略过于确定），α 会增大，增加熵正则化项的权重，鼓励策略进行更多的探索。α 的值直接影响 Actor 损失的优化方向。具体来说，Actor 损失中的熵正则化项 αlogπ(a∣s) 的权重由 α 决定。因此，α 的动态调整能够平衡 Actor 网络在探索和利用之间的权衡。
- logπ(a∣s) 是策略网络在状态 s 下采样出的动作 a 的对数概率。    
- target_entropy 是目标熵值，通常设置为 −action_dim。

1. 为什么 α 要自适应  
	在最大熵 RL（包括 SAC）里，目标函数里有一项 $−α 𝔼[log π(a|s)]$。
	- α 太大 → 策略被迫保持高熵，过度探索，回报下降。
	- α 太小 → 策略趋向确定性，失去探索，易陷入局部最优。  
	    手动调 α 非常痛苦，于是 SAC 把 α 也当成**可训练参数**，让算法自己把策略的**平均熵**拉到用户设定的“目标熵”附近。
2. 数学表述
	1. 优化目标（整体）：  
	    $max_π 𝔼[ Σ_t r_t + α ℋ(π(·|s_t)) ]$  
	    其中 $ℋ(π) = −𝔼_{a∼π}[log π(a|s)]$。
	2. 把 α 也纳入优化：  
	    $min_{α>0} 𝔼[ α ( −log π(a|s) − target\_entropy ) ]$  
	    等价于  
	    $min_{α>0} α · ( 𝔼[ −log π ] − target\_entropy ).$
	3. 目标熵 target_entropy 的选取  
	    连续动作空间：$target\_entropy = −dim(A)$  
	    离散动作空间：$target\_entropy = −log |A|$  
	    （经验值，保证策略不会太随机也不会太集中。）
